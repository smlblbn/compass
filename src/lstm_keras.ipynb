{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(783)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "\n",
    "from keras.optimizers import RMSprop as rm\n",
    "from keras.optimizers import Adagrad as ada\n",
    "from keras.optimizers import SGD as sgd\n",
    "\n",
    "with open('../data/general/matches_2017_2018_v1.json') as matches_json:\n",
    "    matches = pd.read_json(matches_json)\n",
    "\n",
    "data_height = 6000\n",
    "data_width = 87\n",
    "label_team_width = 3\n",
    "label_player_width = 29\n",
    "\n",
    "mean = 0\n",
    "std = 0\n",
    "min_values = 0\n",
    "max_values = 0\n",
    "\n",
    "# min-max scaling\n",
    "def min_max_scaling(data):\n",
    "    global min_values\n",
    "    global max_values\n",
    "    min_values = np.amin(data, axis=(0,1), keepdims=True)\n",
    "    max_values = np.amax(data, axis=(0,1), keepdims=True)\n",
    "    return (data-min_values)/(max_values-min_values)\n",
    "\n",
    "# normalizing\n",
    "def normalize(data):\n",
    "    global mean\n",
    "    global std\n",
    "    mean = np.mean(data, axis=(0,1), keepdims=True)\n",
    "    std = np.std(data, axis=(0,1), keepdims=True)\n",
    "    return (data - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small data arrays (5 matches - 4 of them train, 1 of them validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_small_match_id = 60561\n",
    "small_match_count = 5\n",
    "timestep = 5\n",
    "\n",
    "x_small = np.ndarray(shape=(small_match_count, data_height, data_width), dtype=np.float)\n",
    "y_team_small = np.ndarray(shape=(small_match_count, data_height, label_team_width), dtype=np.int)\n",
    "y_player_small = np.ndarray(shape=(small_match_count, data_height, label_player_width), dtype=np.int)\n",
    "x_pass_small = None\n",
    "y_pass_small = None\n",
    "\n",
    "for id in range(small_match_count):\n",
    "    x_small[id] = np.load('../data/match_' + str(id + first_small_match_id) + '/x_data.npy')\n",
    "    y_team_small[id] = np.load('../data/match_' + str(id + first_small_match_id) + '/y_team_data.npy')\n",
    "    y_player_small[id] = np.load('../data/match_' + str(id + first_small_match_id) + '/y_player_data.npy')\n",
    "    \n",
    "    y_pass = np.load('../data/match_' + str(id + first_small_match_id) + '/y_pass_data.npy')\n",
    "    y_time = np.load('../data/match_' + str(id + first_small_match_id) + '/y_pass_time.npy')\n",
    "    x_pass = np.ndarray(shape=(y_pass.shape[0], timestep, data_width + label_team_width + label_player_width))\n",
    "    \n",
    "    for i, t in enumerate(y_time):\n",
    "        x_pass[i] = np.concatenate((x_small[id][t-timestep:t], y_team_small[id][t-timestep:t], \\\n",
    "                                    y_player_small[id][t-timestep:t]), axis=1)\n",
    "    if x_pass_small is None and y_pass_small is None:\n",
    "        x_pass_small = x_pass\n",
    "        y_pass_small = y_pass\n",
    "    else:\n",
    "        x_pass_small = np.concatenate((x_pass_small, x_pass), axis=0)\n",
    "        y_pass_small = np.concatenate((y_pass_small, y_pass), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_small = x_small[:, 60:5460, :]\n",
    "y_team_small = y_team_small[:, 60:5460, :]\n",
    "y_player_small = y_player_small[:, 60:5460, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_small = normalize(x_small)\n",
    "\n",
    "with open('../model/mean_std.pkl', 'wb') as file:\n",
    "    pickle.dump((mean, std), file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "x_pass_small = normalize(x_pass_small)\n",
    "\n",
    "with open('../model/mean_std_pass.pkl', 'wb') as file:\n",
    "    pickle.dump((mean, std), file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_small = min_max_scaling(x_small)\n",
    "\n",
    "with open('../model/min_max.pkl', 'wb') as file:\n",
    "    pickle.dump((min_values, max_values), file, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "x_pass_small = min_max_scaling(x_pass_small)\n",
    "\n",
    "with open('../model/min_max_pass.pkl', 'wb') as file:\n",
    "    pickle.dump((min_values, max_values), file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(x_small.shape[0])\n",
    "x_small = x_small[permutation]\n",
    "y_team_small = y_team_small[permutation]\n",
    "y_player_small = y_player_small[permutation]\n",
    "\n",
    "permutation = np.random.permutation(x_pass_small.shape[0])\n",
    "x_pass_small = x_pass_small[permutation]\n",
    "y_pass_small = y_pass_small[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_80 = int(x_small.shape[0]*0.8)\n",
    "\n",
    "x_train_small = x_small[:size_80]\n",
    "x_test_small = x_small[size_80:]\n",
    "\n",
    "y_train_team_small = y_team_small[:size_80]\n",
    "y_test_team_small = y_team_small[size_80:]\n",
    "\n",
    "y_train_player_small = y_player_small[:size_80]\n",
    "y_test_player_small = y_player_small[size_80:]\n",
    "\n",
    "size_80 = int(x_pass_small.shape[0]*0.8)\n",
    "\n",
    "x_train_pass_small = x_pass_small[:size_80]\n",
    "x_test_pass_small = x_pass_small[size_80:]\n",
    "\n",
    "y_train_pass_small = y_pass_small[:size_80]\n",
    "y_test_pass_small = y_pass_small[size_80:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data arrays (304 match - 243 of them train, 61 of them validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/match_60561/y_player_time.npy'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-16bb1310e5ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0my_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/match_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/y_player_data.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0my_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/match_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/y_player_time.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mx_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_width\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabel_team_width\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabel_player_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/project/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/match_60561/y_player_time.npy'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "timestep = 5\n",
    "\n",
    "x_all = np.ndarray(shape=(len(matches['id']), data_height, data_width))\n",
    "y_team_all = np.ndarray(shape=(len(matches['id']), data_height, label_team_width))\n",
    "y_player_all = np.ndarray(shape=(len(matches['id']), data_height, label_player_width))\n",
    "x_pass_all = None\n",
    "y_pass_all = None\n",
    "\n",
    "idx = 0\n",
    "for id in matches['id']:\n",
    "    x_all[idx] = np.load('../data/match_' + str(id) + '/x_data.npy')\n",
    "    y_team_all[idx] = np.load('../data/match_' + str(id) + '/y_team_data.npy')\n",
    "    y_player_all[idx] = np.load('../data/match_' + str(id) + '/y_player_data.npy')\n",
    "    \n",
    "    y_pass = np.load('../data/match_' + str(id) + '/y_player_data.npy')\n",
    "    y_time = np.load('../data/match_' + str(id) + '/y_player_time.npy')\n",
    "    x_pass = np.ndarray(shape=(y_pass.shape[0], timestep, data_width + label_team_width + label_player_width))\n",
    "    \n",
    "    for i, t in enumerate(y_time):\n",
    "        x_pass[i] = np.concatenate((x_small[id][t-timestep:t], y_team_small[id][t-timestep:t], \\\n",
    "                                    y_player_small[id][t-timestep:t]), axis=1)\n",
    "    if x_pass_all is None and y_pass_all is None:\n",
    "        x_pass_all = x_pass\n",
    "        y_pass_all = y_pass\n",
    "    else:\n",
    "        x_pass_all = np.concatenate((x_pass_all, x_pass), axis=0)\n",
    "        y_pass_all = np.concatenate((y_pass_all, y_pass), axis=0)\n",
    "    idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = x_all[:, 60:5460, :]\n",
    "y_team_all = y_team_all[:, 60:5460, :]\n",
    "y_player_all = y_player_all[:, 60:5460, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = normalize(x_all)\n",
    "\n",
    "with open('../model/mean_std.pkl', 'wb') as file:\n",
    "    pickle.dump((mean, std), file, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "x_pass_all = normalize(x_pass_all)\n",
    "\n",
    "with open('../model/mean_std_pass.pkl', 'wb') as file:\n",
    "    pickle.dump((mean, std), file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = min_max_scaling(x_all)\n",
    "\n",
    "with open('../model/min_max.pkl', 'wb') as file:\n",
    "    pickle.dump((min_values, max_values), file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "x_pass_all = min_max_scaling(x_pass_all)\n",
    "\n",
    "with open('../model/min_max_pass.pkl', 'wb') as file:\n",
    "    pickle.dump((min_values, max_values), file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(x_all.shape[0])\n",
    "x_all = x_all[permutation]\n",
    "y_team_sall = y_team_all[permutation]\n",
    "y_player_all = y_player_all[permutation]\n",
    "\n",
    "permutation = np.random.permutation(x_pass_all.shape[0])\n",
    "x_pass_all = x_pass_all[permutation]\n",
    "y_pass_all = y_pass_all[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_80 = int(x_all.shape[0]*0.8)\n",
    "\n",
    "x_train_all = x_all[:size_80]\n",
    "x_test_all = x_all[size_80:]\n",
    "\n",
    "y_train_team_all = y_team_all[:size_80]\n",
    "y_test_team_all = y_team_all[size_80:]\n",
    "\n",
    "y_train_player_all = y_player_all[:size_80]\n",
    "y_test_player_all = y_player_all[size_80:]\n",
    "\n",
    "size_80 = int(x_pass_all.shape[0]*0.8)\n",
    "\n",
    "x_train_pass_all = x_pass_all[:size_80]\n",
    "x_test_pass_all = x_pass_all[size_80:]\n",
    "\n",
    "y_train_pass_all = y_pass_all[:size_80]\n",
    "y_test_pass_all = y_pass_all[size_80:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team ball possesion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3 samples, validate on 1 samples\nEpoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 20s - loss: 1.3111 - acc: 0.3936 - val_loss: 1.6596 - val_acc: 0.4207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 18s - loss: 0.7815 - acc: 0.6769 - val_loss: 1.4312 - val_acc: 0.4415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 16s - loss: 0.6097 - acc: 0.7723 - val_loss: 1.4049 - val_acc: 0.4559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 16s - loss: 0.5214 - acc: 0.8188 - val_loss: 1.3835 - val_acc: 0.4406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 17s - loss: 0.4706 - acc: 0.8422 - val_loss: 1.4183 - val_acc: 0.4569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 17s - loss: 0.4433 - acc: 0.8470 - val_loss: 1.3815 - val_acc: 0.4313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 18s - loss: 0.3909 - acc: 0.8726 - val_loss: 1.3655 - val_acc: 0.4456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-4a7db0f911c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m history_team = model_team.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=2, callbacks=None, \n\u001b[1;32m     40\u001b[0m                               \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                               sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/project/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/project/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/project/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/project/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/project/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "x = x_train_small\n",
    "#x = x_train_all\n",
    "\n",
    "y = y_train_team_small\n",
    "#y = y_train_team_all\n",
    "\n",
    "timestep = 5400\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 100\n",
    "\n",
    "input_dim = 87\n",
    "output_dim = 3\n",
    "\n",
    "model_team = Sequential()\n",
    "model_team.add(LSTM(300, \n",
    "               input_shape=(timestep, input_dim), \n",
    "               return_sequences=True,\n",
    "               kernel_initializer=initializers.he_normal(783)))\n",
    "model_team.add(BatchNormalization())\n",
    "model_team.add(Dense(200, kernel_initializer=initializers.he_normal(783), activation='relu'))\n",
    "model_team.add(BatchNormalization())\n",
    "model_team.add(Dense(100, kernel_initializer=initializers.he_normal(783), activation='relu'))\n",
    "model_team.add(BatchNormalization())\n",
    "model_team.add(Dense(50, kernel_initializer=initializers.he_normal(783), activation='relu'))\n",
    "model_team.add(BatchNormalization())\n",
    "model_team.add(Dense(25, kernel_initializer=initializers.he_normal(783), activation='relu'))\n",
    "model_team.add(BatchNormalization())\n",
    "model_team.add(Dense(output_dim, kernel_initializer=initializers.he_normal(783), activation='softmax'))\n",
    "\n",
    "opt = rm()\n",
    "#opt = ada()\n",
    "#opt = sgd(lr=0.005, momentum=0.9, decay=1e-5, nesterov=True)\n",
    "\n",
    "model_team.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=opt,\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "history_team = model_team.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=2, callbacks=None, \n",
    "                              validation_split=0.2, validation_data=None, shuffle=True, class_weight=None, \n",
    "                              sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test_small\n",
    "#x_test = x_test_all\n",
    "\n",
    "y_test = y_test_team_small\n",
    "#y_test = y_test_team_all\n",
    "\n",
    "score = model_team.evaluate(x_test, y_test)\n",
    "print('team possesion test acc: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_team_pred = model_team.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history_team.history['acc'])\n",
    "plt.plot(history_team.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.savefig('../graphs/team_acc.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history_team.history['loss'])\n",
    "plt.plot(history_team.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.savefig('../graphs/team_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_team.save('../model/team_model.h5')\n",
    "model_team.save_weights('../model/team_model_weight.h5')\n",
    "json_data = model_team.to_json()\n",
    "\n",
    "with open('../model/team_json_data.txt', 'w') as outfile:\n",
    "    json.dump(json_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Player ball possesion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3 samples, validate on 1 samples\nEpoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 20s - loss: 3.8284 - acc: 0.1059 - val_loss: 3.8446 - val_acc: 0.1613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 16s - loss: 2.7308 - acc: 0.3522 - val_loss: 3.6835 - val_acc: 0.1709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 17s - loss: 2.2721 - acc: 0.4600 - val_loss: 3.5844 - val_acc: 0.1802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 18s - loss: 2.0151 - acc: 0.5217 - val_loss: 3.4594 - val_acc: 0.2087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 18s - loss: 1.8664 - acc: 0.5590 - val_loss: 3.5679 - val_acc: 0.1865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 20s - loss: 1.7767 - acc: 0.5754 - val_loss: 3.2683 - val_acc: 0.2748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 18s - loss: 1.6329 - acc: 0.6151 - val_loss: 3.4195 - val_acc: 0.2231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n"
     ]
    }
   ],
   "source": [
    "x = np.concatenate((x_train_small, y_train_team_small), axis=2)\n",
    "#x = np.concatenate((x_train_all, y_train_team_all), axis=2)\n",
    "\n",
    "y = y_train_player_small\n",
    "#y = y_train_player_all\n",
    "\n",
    "timestep = 5400\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 100\n",
    "\n",
    "input_dim = 90\n",
    "output_dim = 29\n",
    "\n",
    "model_player = Sequential()\n",
    "model_player.add(LSTM(300, \n",
    "               input_shape=(timestep, input_dim), \n",
    "               return_sequences=True,\n",
    "               kernel_initializer=initializers.he_normal(783)))\n",
    "model_player.add(BatchNormalization())\n",
    "model_player.add(Dense(200, kernel_initializer=initializers.he_normal(783), activation='relu'))\n",
    "model_player.add(BatchNormalization())\n",
    "model_player.add(Dense(100, kernel_initializer=initializers.he_normal(783), activation='relu'))\n",
    "model_player.add(BatchNormalization())\n",
    "model_player.add(Dense(50, kernel_initializer=initializers.he_normal(783), activation='relu'))\n",
    "model_player.add(BatchNormalization())\n",
    "model_player.add(Dense(output_dim, kernel_initializer=initializers.he_normal(783), activation='softmax'))\n",
    "\n",
    "opt = rm()\n",
    "#opt = ada()\n",
    "#opt = sgd(lr=0.005, momentum=0.9, decay=1e-5, nesterov=True)\n",
    "\n",
    "model_player.compile(loss='categorical_crossentropy',\n",
    "                     optimizer=opt,\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "history_player = model_player.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=2, callbacks=None, \n",
    "                                  validation_split=0.2, validation_data=None, shuffle=True, class_weight=None, \n",
    "                                  sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.concatenate((x_test_small, y_test_team_small), axis=2)\n",
    "#x_test = np.concatenate((x_test_all, y_test_team_all), axis=2)\n",
    "\n",
    "y_test = y_test_player_small\n",
    "#y_test = y_test_player_all\n",
    "\n",
    "score = model_player.evaluate(x_test, y_test)\n",
    "print('team possesion test acc: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_player_pred = model_player.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history_player.history['acc'])\n",
    "plt.plot(history_player.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.savefig('../graphs/player_acc.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history_player.history['loss'])\n",
    "plt.plot(history_player.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.savefig('../graphs/player_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_player.save('../model/player_model.h5')\n",
    "model_player.save_weights('../model/player_model_weight.h5')\n",
    "json_data = model_player.to_json()\n",
    "\n",
    "with open('../model/player_json_data.txt', 'w') as outfile:\n",
    "    json.dump(json_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x_train_pass_small\n",
    "#x = x_train_pass_all\n",
    "\n",
    "y = y_train_pass_small\n",
    "#y = y_train_pass_all\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "timestep = 5\n",
    "\n",
    "input_dim = 119\n",
    "output_dim = 28\n",
    "\n",
    "model_pass = Sequential()\n",
    "model_pass.add(LSTM(300, \n",
    "               input_shape=(timestep, input_dim), \n",
    "               return_sequences=True,\n",
    "               kernel_initializer=initializers.he_normal(783)))\n",
    "model_pass.add(LSTM(600, \n",
    "               input_shape=(timestep, input_dim), \n",
    "               return_sequences=False,\n",
    "               kernel_initializer=initializers.he_normal(783)))\n",
    "model_pass.add(BatchNormalization())\n",
    "model_pass.add(Dense(400, kernel_initializer=initializers.he_normal(783), activation='relu'))\n",
    "model_pass.add(BatchNormalization())\n",
    "model_pass.add(Dense(200, kernel_initializer=initializers.he_normal(783), activation='relu'))\n",
    "model_pass.add(BatchNormalization())\n",
    "model_pass.add(Dense(100, kernel_initializer=initializers.he_normal(783), activation='relu'))\n",
    "model_pass.add(BatchNormalization())\n",
    "model_pass.add(Dense(50, kernel_initializer=initializers.he_normal(783), activation='relu'))\n",
    "model_pass.add(BatchNormalization())\n",
    "model_pass.add(Dense(output_dim, kernel_initializer=initializers.he_normal(783), activation='softmax'))\n",
    "\n",
    "opt = rm()\n",
    "#opt = ada()\n",
    "#opt = sgd(lr=0.005, momentum=0.9, decay=1e-5, nesterov=True)\n",
    "\n",
    "model_pass.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=opt,\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "history_pass = model_pass.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=2, callbacks=None, \n",
    "                              validation_split=0.2, validation_data=None, shuffle=True, class_weight=None, \n",
    "                              sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test_pass_small\n",
    "#x_test = x_test_pass_all\n",
    "\n",
    "y_test = y_test_pass_small\n",
    "#y_test = y_test_pass_all\n",
    "\n",
    "score = model_pass.evaluate(x_test, y_test)\n",
    "print('team possesion test acc: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pass_pred = model_pass.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history_pass.history['acc'])\n",
    "plt.plot(history_pass.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.savefig('../graphs/pass_acc.png')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history_pass.history['loss'])\n",
    "plt.plot(history_pass.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.savefig('../graphs/pass_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pass.save('../model/pass_model.h5')\n",
    "model_pass.save_weights('../model/pass_model_weight.h5')\n",
    "json_data = model_pass.to_json()\n",
    "\n",
    "with open('../model/pass_json_data.txt', 'w') as outfile:\n",
    "    json.dump(json_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
